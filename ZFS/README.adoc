= ZFS

An introduction to ZFS (what it is) can be found https://itsfoss.com/what-is-zfs/[here].

vdev is a virtual device built on top of disks, partitions, files or LUNs. Within the pool, on top of vdevs, is a file system. Data is automatically balanced across all vdevs, across all disks.

== Setup environment
* Create a new Ubuntu 20.04-server VM (not via Vagrant)
* Is the command zfs available? 
** Yes, OK ready to continue
** No, install zfsutils-linux after an "apt update"
* Add three new disks to the VM each 5GB in size
** reboot the system
* Run `lsblk` to get a list of all installed block devices
** You will see three disk of 2GB in size, write down their names

== Create a Pool
The most common pool configurations are stripe, mirror, raidz and raidz2, choose one from the following:
* stripe pool (similar to raid-0, no redundancy): `zpool create tank sdc sdd`
* mirror pool (similar to raid-1, ≥ 2 disks, 1:1 redundancy): `zpool create tank mirror sdc sdd`
* raidz1 pool (similar to raid-5, ≥ 3 disks, 1 disk redundancy): `zpool create tank raidz sdc sdd sde`
* raidz2 pool (similar to raid-6, ≥ 4 disks, 2 disks redundancy): `zpool create tank raidz2 sdc sdd sde sdf`
* raidz3 pool ... to advanced!

=== Exercise
* Create a striped ZFS pool containing the three disks. Name the pool "tank"  
** The newly created pool is mounted at /tank
** the mount point can be changed with the -m option
** a pool can be destroyed with `zpool destroy {pool name}`
* Destroy pool tank and create a new one on /mnt/zfs/tank
** `zpool create -m /mnt/zfs/tank sdb sdc sdd`
* Check the pool status with `zpool status`

== Advanced pool configuration
If building a pool with a larger number of disks, you are encouraged to configure them into more than one group. This would allow more flexible pool design to trade-off among space, redundancy and efficiency.

* 5 mirror (like raid-10, 1:1 redundancy): `zpool create tank mirror sdc sdd mirror sde sdf mirror sdg sdh mirror sdi sdj mirror sdk sdl`

* 2 raidz (like raid-50, 2 disks redundancy in total): `zpool create tank raidz sdc sdd sde sdf sdg raidz sdh sdi sdj sdk sdl

ZFS can make use of fast SSD as second level cache (L2ARC) after RAM (ARC), which can improve cache hit rate thus improving overall performance. Because cache devices could be read and write very frequently when the pool is busy, please consider to use more durable SSD devices (SLC/MLC over TLC/QLC) preferably come with NVMe protocol. This cache is only use for read operations, so that data write to cache disk is demanded by read operations, and is not related to write operations at all.

    zpool add tank cache nvme-ssd-disk

== Provisioning file systems or volume
After creating the zpool, we are able to provision file systems or volumes (ZVOL). ZVOL is a kind of block device whose space is allocated from the pool. It is possible to create another file system on it (including swap) or use it as storage device for virtual machines, like any other block device.

* provision a file system named data under pool tank, and have it mounted on /data 

    mkdir -p /data
    zfs create -o mountpoint=/data tank/data

* thin provision a ZVOL of 4GB named vol under pool tank, and format it to ext4, then mount on /mnt temporarily 

    zfs create -s -V 4GB tank/vol
    mkfs.ext4 /dev/zvol/tank/vol
    mount /dev/zvol/tank/vol /mnt

* destroy previously created file systems and ZVOL 

    umount /data
    zfs destroy tank/data
    umount /dev/zvol/tank/vol
    zfs destroy tank/vol

== Snapshots: Creating and Managing Snapshots

* making a snapshot of tank/data 

    zfs snapshot tank/data@2019-06-24

* list the snapshots

    zfs list -t snapshot
    zfs list -r -t snapshot -o name,creation


* removing a snapshot 

    zfs destroy tank/data@2019-06-24


== Backup and Restore (with remote)

It is possible to backup a ZFS dataset to another pool with zfs send/recv commands, even the pool is located at the other end of network.

    # create a initial snapshot
    zfs snapshot tank/data@initial
    # send it to another local pool, named ''tank2'', and calling the dataset ''packman''
    zfs send tank/data@initial | zfs recv -F tank2/packman
    # send it to a remote pool, named ''tanker'' at remote side
    zfs send tank/data@initial | ssh remotehost zfs recv -F tanker/data
    # after using ''tank/data'' for a while, create another snapshot
    zfs snapshot tank/data@2019-06-24T18-10
    # incrementally send the new state to remote
    zfs send -i initial tank/data@2019-06-24T18-10 | ssh remotehost zfs recv -F tanker/data

== Advanced Topics

These are not really advanced stuff like internals of ZFS and storage, but are some topics not relevant to everyone.

* 64-bit hardware and kernel is recommended. ZFS wants a lot of memory (so as address space) to work best, also it was developed with an assumption of being 64-bit only from the beginning. It is possible to use ZFS under 32-bit environments but a lot of care must be taken by the user.

* Use ashift=12 or ashift=13 when creating the pool if applicable (though ZFS can detect correctly for most cases). Value of ashift is exponent of 2, which should be aligned to the physical sector size of disks, for example 29=512, 212=4096, 213=8192. Some disks are reporting a logical sector size of 512 bytes while having 4KiB physical sector size (aka 512e), and some SSDs have 8KiB physical sector size. USB racks can also prevent the correct detection of the physical sector size. Consider using ashift=12 or ashift=13 even if currently using only disks with 512 bytes sectors. Adding devices with bigger sectors to the same VDEV can severely impact performance due to wrong alignment, while a device with 512 sectors will work also with a higher ashift (bigger sectors will be aligned as they are multiples of 512).

    zpool create -o ashift=12 tank mirror sdc sdd

* Enable compression if not absolutely paranoid because ZFS can skip compression of objects that it sees not effect, and compressed objects can improve IO efficiency. This will enable compression using the current default compression algorithm (currently lz4 if the pool has the lz4_compress feature enabled).

    zfs set compression=on tank

* Install as much RAM as feasible. ZFS has advanced caching design which could take advantage of a lot of memory to improve performance. This cache is called Adjustable Replacement Cache (ARC). Block level deduplication is scary when RAM is limited, but such feature is getting increasingly promoted on professional storage solutions nowadays, since it could perform impressively for scenarios like storing VM disks that share common ancestors. Because deduplication table is part of ARC, it's possible to use a fast L2ARC (NVMe SSD) to mitigate the problem of lacking RAM. Typical space requirement would be 2-5 GB ARC/L2ARC for 1TB of disk, if you are building a storage of 1PB raw capacity, at least 1TB of L2ARC space should be planned for deduplication (minimum size, assuming pool is mirrored). 

    # dragons ahead, you have been warned
    zfs set dedup=on tank/data

== What you’ll learn
* How to install ZFS on Ubuntu
* How to create a storage pool


== Reference 
* https://ubuntu.com/tutorials/setup-zfs-storage-pool
* https://wiki.debian.org/ZFS
* https://pve.proxmox.com/wiki/ZFS_on_Linux